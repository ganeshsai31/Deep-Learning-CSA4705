# ================================================================
# Constructing and Verifying Multi-Layer Perceptron (MLP) Model
# with Different Inputs, Learning Rate, and Activation Functions
# ================================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# -------------------------------
# STEP 1: Generate Input Dataset
# -------------------------------
# You can try 'moons', 'circles', or 'linear'
X, y = make_moons(n_samples=1000, noise=0.25, random_state=42)

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize input data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# -------------------------------
# STEP 2: Define a Function to Create MLP Models
# -------------------------------
def create_mlp(activation='relu', learning_rate=0.01):
    model = Sequential([
        Dense(8, input_dim=2, activation=activation),
        Dense(8, activation=activation),
        Dense(1, activation='sigmoid')
    ])
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# -------------------------------
# STEP 3: Train MLP Models with Different Settings
# -------------------------------
configs = [
    ('relu', 0.01),
    ('tanh', 0.01),
    ('sigmoid', 0.01),
    ('relu', 0.1),
    ('tanh', 0.1)
]

results = {}

for act, lr in configs:
    print(f"\nTraining Model with Activation={act}, Learning Rate={lr}")
    model = create_mlp(activation=act, learning_rate=lr)
    history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0,
                        validation_data=(X_test, y_test))
    results[(act, lr)] = history

# -------------------------------
# STEP 4: Visualize Performance
# -------------------------------
plt.figure(figsize=(12,8))
for (act, lr), history in results.items():
    plt.plot(history.history['val_accuracy'], label=f'{act}, lr={lr}')
plt.title('Validation Accuracy for Different Activations and Learning Rates')
plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.legend()
plt.show()

# -------------------------------
# STEP 5: Verify Model on Test Data
# -------------------------------
for (act, lr), history in results.items():
    final_acc = history.history['val_accuracy'][-1]
    print(f"Activation: {act:7s} | Learning Rate: {lr:.3f} | Final Accuracy: {final_acc:.3f}")
