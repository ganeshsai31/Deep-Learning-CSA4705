# ===========================================================
# DEMONSTRATION OF GRADIENT DESCENT ALGORITHM IN PYTHON
# ===========================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# -----------------------------------------
# STEP 1: Create a simple numerical dataset
# -----------------------------------------
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)  # Linear relation with noise

# Visualize dataset
plt.figure(figsize=(6,4))
plt.scatter(X, y, color='blue', alpha=0.6)
plt.title("Input Data for Gradient Descent (y = 4 + 3x + noise)")
plt.xlabel("X")
plt.ylabel("y")
plt.grid(True)
plt.show()

# -----------------------------------------
# STEP 2: Implement Gradient Descent Algorithm
# -----------------------------------------

# Add bias term (x0 = 1)
X_b = np.c_[np.ones((100, 1)), X]

# Initialize parameters (theta0, theta1)
theta = np.random.randn(2,1)

# Learning rate and iterations
learning_rate = 0.1
n_iterations = 100
m = len(X_b)

# Store cost history
cost_history = []

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - learning_rate * gradients
    cost = np.mean((X_b.dot(theta) - y) ** 2)
    cost_history.append(cost)

print("\n===== FINAL PARAMETERS FROM GRADIENT DESCENT =====")
print(f"Theta0 (intercept): {theta[0][0]:.3f}")
print(f"Theta1 (slope): {theta[1][0]:.3f}")

# -----------------------------------------
# STEP 3: Plot Cost Function Convergence
# -----------------------------------------
plt.figure(figsize=(6,4))
plt.plot(range(n_iterations), cost_history, color='red')
plt.title("Gradient Descent: Cost Function Convergence")
plt.xlabel("Number of Iterations")
plt.ylabel("Mean Squared Error (Cost)")
plt.grid(True)
plt.show()

# -----------------------------------------
# STEP 4: Compare with scikit-learn Linear Regression
# -----------------------------------------
lin_reg = LinearRegression()
lin_reg.fit(X, y)

print("\n===== COMPARISON WITH SKLEARN LINEAR REGRESSION =====")
print(f"Sklearn Intercept: {lin_reg.intercept_[0]:.3f}")
print(f"Sklearn Coefficient: {lin_reg.coef_[0][0]:.3f}")

# -----------------------------------------
# STEP 5: Visualize Regression Line
# -----------------------------------------
plt.figure(figsize=(6,4))
plt.scatter(X, y, color='blue', label="Data Points", alpha=0.6)
plt.plot(X, X_b.dot(theta), color='red', label="Gradient Descent Fit")
plt.plot(X, lin_reg.predict(X), color='green', linestyle='--', label="Sklearn Fit")
plt.title("Gradient Descent vs Linear Regression Fit")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.grid(True)
plt.show()
